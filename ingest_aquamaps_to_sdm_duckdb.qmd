---
title: "Ingest AquaMaps to SDM DuckDB"
subtitle: "Transform downscaled AquaMaps data into DuckDB for efficient biodiversity analysis"
format:
  html:
    code-fold: true
    code-tools: true
editor_options: 
  chunk_output_type: console
---

## Overview

This notebook ingests downscaled AquaMaps species distribution models (0.05° resolution) into a DuckDB database (`msens_sdm`) for efficient querying and biodiversity metric calculation. The database schema supports multiple SDM sources and includes spatial functions for raster generation and polygon summarization.

```{r setup}
#| message: false
#| warning: false

librarian::shelf(
  DBI, dbplyr, dplyr, duckdb, fs, glue, here, leaflet, logger, mapgl, purrr, readr,
  sf, stringr, terra, tibble, tidyr, quiet = T )
options(readr.show_col_types = F)

# b/c: [rotate() shifts cells of non-global spatRast](https://github.com/rspatial/terra/issues/1831)
# remotes::install_github("rspatial/terra")
# devtools::session_info()  # 2025-05-28 [1] Github (rspatial/terra@06ee3b1)
stopifnot(packageVersion("terra") >= "1.8.53")

# source(here("libs/db.R"))
source(here("libs/am_functions.R"))
source(here("libs/sdm_functions.R"))

# set up directories
is_server <-  Sys.info()[["sysname"]] == "Linux"
dir_data <- ifelse(
  is_server,
  "/share/data",
  "~/My Drive/projects/msens/data")

# database setup
# db_path <- glue("{dir_data}/derived/msens_sdm.duckdb") # OLD: 10 GB with indexes, unvacuumed data, etc

# file_delete(sdm_dd)
sdm_db <- glue("{dir_data}/derived/sdm.duckdb")
con_sdm <- dbConnect(duckdb(), dbdir = sdm_db, read_only = F) |> 
  load_duckdb_extensions()
# dbDisconnect(con_sdm, shutdown = T)
# duckdb_shutdown(duckdb())
# dbListTables(con_sdm)
dbExecute(con_sdm, "SET threads = 6;")
dbExecute(con_sdm, "SET memory_limit = '10GB';")

con_am   <- dbConnect(    # con_dd to AquaMaps duckdb
  duckdb(
    dbdir     = glue("{dir_data}/derived/aquamaps/am.duckdb"),
    read_only = T))
```

## Database Schema

The database schema supports multiple species per model and includes biodiversity metrics with flexible regional aggregation.

```{mermaid}
erDiagram
%% https://mermaid.js.org/syntax/entityRelationshipDiagram.html

%% tables
dataset {
  str ds_key          PK  "dataset key"
  str name_short          "short name: {source_broad} {taxa_groups} {response_type} in {regions}, {year_pub}"
  str name_original       "original dataset name"
  str description
  str citation
  str source_broad        "e.g. NOAA, AquaMaps, Duke, NCCOS"
  str source_detail       "e.g. NOAA Southeast Fisheries Science Center"
  str regions             "e.g. Global, Atlantic, Pacific, Gulf of America"
  str response_type       "occurrence, range, suitability, biomass, density, diversity"
  str taxa_groups         "fish, invertebrates, marine mammals, cetaceans, sea turtles, seabirds"
  int year_pub            "year published"
  date date_obs_beg       "first observation date"
  date date_obs_end       "last observation date"
  date date_env_beg       "first environmental data date"
  date date_env_end       "last environmental data date"
  str link_info           "primary information URL"
  str link_download       "download URL"
  str link_metadata       "metadata URL"
  str links_other         "other URLs space-separated"
  dbl spatial_res_deg     "spatial resolution in decimal degrees"
  str temporal_res        "temporal resolution"
  date date_created       "date added to database"
}

model {
  int mdl_seq         PK  "sequential model ID"
  str ds_key          FK  "dataset key"
  str taxa                "taxonomic identifier"
  str time_period         "time period"
  str region              "geographic region"
  str mdl_type            "occurrence, range, suitability, biomass, density, diversity"
  str description
  date date_created       "date added to database"
}

species {
  int sp_seq          PK  "sequential species ID"
  str ds_key          FK  "dataset key"
  str taxa            FK  "links to model.taxa"
  str sp_key              "species key"
  int aphia_id            "WoRMS Aphia ID"
  int gbif_id             "GBIF ID"
  int itis_id             "ITIS ID"
  str scientific_name
  str common_name
  str sp_cat              "fish, mollusk, crustacean, etc."
}

cell {
  int cell_id         PK  "cell ID"
  dbl lon                 "longitude"
  dbl lat                 "latitude"
  dbl area_km2            "cell area in square kilometers"
  geo geom                "cell geometry"
}

model_cell {
  int cell_id         FK  "cell ID"
  int mdl_seq         FK  "model sequential ID"
  dbl value               "model value"
}

metric {
  int metric_seq      PK  "auto-incrementing metric ID"
  str metric_type         "species_richness, shannon_index, etc."
  str description
  date date_created       "date metric was created"
}

cell_metric {
  int cell_id         FK  "cell ID"
  int metric_seq      FK  "metric ID"
  dbl value               "calculated metric value"
}

region {
  int rgn_seq         PK  "auto-incrementing region ID"
  str tbl_col             "{table}.{column} e.g. planning_areas.pa_key"
  str value               "unique identifier value e.g. CHU"
  str col_geom            "geometry column name"
  date date_created       "date added to database"
}

region_metric {
  int rgn_seq         FK  "region ID"
  int metric_seq      FK  "metric ID"
  dbl value               "calculated metric value"
}

region_cell {
  int rgn_seq         FK  "region ID"
  int cell_id         FK  "cell ID"
  int pct_covered         "percentage of cell covered by region"
}

planning_areas {
  str pa_key          PK  "planning area key"
  str pa_name
  geo geom                "planning area geometry"
}

%% relationships
dataset ||--o{ model : ds_key
dataset ||--o{ species : ds_key
model ||--|{ species : "ds_key,taxa"
model ||--o{ model_cell : mdl_seq
cell ||--o{ model_cell : cell_id
cell ||--o{ cell_metric : cell_id
cell ||--o{ region_cell : cell_id
metric ||--o{ cell_metric : metric_seq
metric ||--o{ region_metric : metric_seq
region ||--o{ region_metric : rgn_seq
region ||--o{ region_cell : rgn_seq
```

## Create Database Tables

```{r create_tables}
#| message: false
#| eval: false

# DO NOT run this chunk if the database already exists

# drop existing tables if needed
if (dbExistsTable(con_sdm, "model_cell") || dbExistsTable(con_sdm, "sdm_values")) {
  tables_to_drop <- c(
    "region_metric", "region_cell", "cell_metric", "env_layers", 
    "model_cell", "sdm_values", "species", "model", "dataset", 
    "cell", "sdm_cells", "metric", "region", "planning_areas",
    "biodiv_metrics", "planning_area_metrics", "sdm_species", 
    "sdm_models", "sdm_sources"
  )
  
  for (tbl in tables_to_drop) {
    if (dbExistsTable(con_sdm, tbl))
      dbExecute(con_sdm, glue("DROP TABLE {tbl} CASCADE"))
  }
}

# create tables
dbExecute(con_sdm, "
  CREATE TABLE dataset (
    ds_key VARCHAR PRIMARY KEY,
    name_short VARCHAR NOT NULL,
    name_original VARCHAR,
    description TEXT,
    citation TEXT,
    source_broad VARCHAR,
    source_detail VARCHAR,
    regions VARCHAR,
    response_type VARCHAR,
    taxa_groups VARCHAR,
    year_pub INTEGER,
    date_obs_beg DATE,
    date_obs_end DATE,
    date_env_beg DATE,
    date_env_end DATE,
    link_info VARCHAR,
    link_download VARCHAR,
    link_metadata VARCHAR,
    links_other VARCHAR,
    spatial_res_deg DOUBLE,
    temporal_res VARCHAR,
    date_created DATE DEFAULT CURRENT_DATE )")

# dbExecute(con_sdm, "DROP TABLE model")
# dbExecute(con_sdm, "DROP SEQUENCE model_mdl_seq_seq CASCADE")
dbExecute(con_sdm, "
  CREATE SEQUENCE model_mdl_seq_seq START 1")

dbExecute(con_sdm, "
  CREATE TABLE model (
    mdl_seq INTEGER PRIMARY KEY DEFAULT nextval('model_mdl_seq_seq'),
    ds_key VARCHAR REFERENCES dataset(ds_key),
    taxa VARCHAR,
    time_period VARCHAR,
    region VARCHAR,
    mdl_type VARCHAR,
    description TEXT,
    date_created DATE DEFAULT CURRENT_DATE )")

# dbExecute(con_sdm, "DROP TABLE species")

dbExecute(con_sdm, "
  CREATE SEQUENCE species_sp_seq_seq START 1")

dbExecute(con_sdm, "
  CREATE TABLE species (
    sp_seq INTEGER PRIMARY KEY DEFAULT nextval('species_sp_seq_seq'),
    ds_key VARCHAR REFERENCES dataset(ds_key),
    taxa VARCHAR,
    sp_key VARCHAR NOT NULL,
    aphia_id INTEGER,
    gbif_id INTEGER,
    itis_id INTEGER,
    scientific_name VARCHAR,
    common_name VARCHAR,
    sp_cat VARCHAR)")

dbExecute(con_sdm, "
  CREATE SEQUENCE metric_metric_seq_seq START 1")

dbExecute(con_sdm, "
  CREATE TABLE metric (
    metric_seq INTEGER PRIMARY KEY DEFAULT nextval('metric_metric_seq_seq'),
    metric_type VARCHAR NOT NULL,
    description TEXT,
    date_created DATE DEFAULT CURRENT_DATE)")

# dbExecute(con_sdm, "
#   CREATE TABLE region (
#     rgn_seq INTEGER PRIMARY KEY DEFAULT nextval('region_rgn_seq_seq'),
#     tbl_col VARCHAR NOT NULL,
#     value VARCHAR NOT NULL,
#     col_geom VARCHAR NOT NULL,
#     date_created DATE DEFAULT CURRENT_DATE )")
# 
# dbExecute(con_sdm, "
#   CREATE SEQUENCE region_rgn_seq_seq START 1")

dbExecute(con_sdm, "
  CREATE SEQUENCE zone_zone_seq_seq START 1")

# dbExecute(con_sdm, "DROP TABLE IF EXISTS zone")
dbExecute(con_sdm, "
  CREATE TABLE zone (
    zone_seq INTEGER PRIMARY KEY DEFAULT nextval('zone_zone_seq_seq'),
    tbl VARCHAR NOT NULL,
    fld VARCHAR NOT NULL,
    value VARCHAR NOT NULL,
    date_created DATE DEFAULT CURRENT_DATE )")

# dbExecute(con_sdm, "
#   CREATE TABLE region_metric (
#     rgn_seq INTEGER REFERENCES region(rgn_seq),
#     metric_seq INTEGER REFERENCES metric(metric_seq),
#     value DOUBLE,
#     PRIMARY KEY (rgn_seq, metric_seq) )")

# dbExecute(con_sdm, "
#   CREATE TABLE zone_metric (
#     zone_seq INTEGER REFERENCES zone(zone_seq),
#     metric_seq INTEGER REFERENCES metric(metric_seq),
#     value DOUBLE,
#     PRIMARY KEY (zone_seq, metric_seq) )")
dbExecute(con_sdm, "
  CREATE TABLE zone_metric (
    zone_seq INTEGER,   -- REFERENCES zone(zone_seq)
    metric_seq INTEGER, -- REFERENCES metric(metric_seq),
    value DOUBLE,
    PRIMARY KEY (zone_seq, metric_seq) )")

# dbExecute(con_sdm, "
# CREATE TABLE cell (
#   cell_id INTEGER PRIMARY KEY,
#   lon DOUBLE NOT NULL,
#   lat DOUBLE NOT NULL,
#   area_km2 DOUBLE NOT NULL,
#   geom GEOMETRY)")

dbExecute(con_sdm, "
CREATE TABLE cell_metric (
  cell_id INTEGER REFERENCES cell(cell_id),
  metric_seq INTEGER REFERENCES metric(metric_seq),
  value DOUBLE,
  PRIMARY KEY (cell_id, metric_seq))")

# dbExecute(con_sdm, "DROP TABLE model_cell")
# dbExecute(con_sdm, "DROP TABLE cell_metric")
# dbExecute(con_sdm, "DROP TABLE region_cell")
# dbExecute(con_sdm, "DROP TABLE cell")

# dbExecute(con_sdm, "
# CREATE TABLE planning_areas (
#   pa_key VARCHAR PRIMARY KEY,
#   pa_name VARCHAR,
#   geom GEOMETRY
# )")
```

## Ingest AquaMaps Source

```{r ingest_source}

# add AquaMaps as a source
aquamaps_dataset <- tibble(
  ds_key          = "am_0.05",
  name_short      = "AquaMaps suitability for all marine taxa (except birds) globally, 2019",
  name_original   = "AquaMaps: Predicted range maps for aquatic species",
  description     = "AquaMaps species distribution models downscaled from 0.5° to 0.05° cells",
  citation        = "Kaschner, K., K. Kesner-Reyes, C. Garilao, J. Segschneider, J. Rius-Barile, T. Rees, and R. Froese. 2019. AquaMaps: Predicted range maps for aquatic species. World wide web electronic publication, www.aquamaps.org, Version 10/2019. Content from AquaMaps as provided in this R package is licensed under a Creative Commons Attribution-NonCommercial 3.0 UnportedLicense, please see http://creativecommons.org/licenses/by-nc/3.0/",
  source_broad    = "AquaMaps",
  source_detail   = "AquaMaps.org",
  regions         = "Global",
  response_type   = "suitability",
  taxa_groups     = "all taxa, except birds",
  year_pub        = 2023,
  date_obs_beg    = NA,
  date_obs_end    = as.Date("2019-01-01"),
  date_env_beg    = NA,
  date_env_end    = as.Date("2019-01-01"),
  link_info       = "https://www.aquamaps.org",
  link_download   = "https://raquamaps.github.io/aquamapsdata",
  link_metadata   = "https://raquamaps.github.io/aquamapsdata/articles/intro.html#data-scope-and-content-1",
  links_other     = NA,
  spatial_res_deg = 0.05,
  temporal_res    = "static" )

if (dbExistsTable(con_sdm, "dataset"))
  dbExecute(con_sdm, "DELETE FROM dataset WHERE ds_key = 'am_0.05'")

dbWriteTable(con_sdm, "dataset", aquamaps_dataset, append = TRUE)
```

```{r tmp_not_evel, include=FALSE}
knitr::opts_chunk$set(eval = F)
```

## Load Planning Areas

Created in PostGIS db `ply_planareas_2025` in "planareas_2025" R chunk of `ingest_aquamaps_res05.qmd`.

```{r skip_load_planning_areas}
# on second thought, skip spatial in duckdb
# dbExecute(con_sdm, "DROP TABLE planning_areas")
#| eval: false

librarian::shelf(
  Cidree/duckspatial, mapview)

if (!dbExistsTable(con_sdm, "ply_planareas")) {
  # read planning areas from existing database
  ply_pa <- st_read(con, "ply_planareas_2025") |> 
    st_break_antimeridian()
  
  ddbs_write_vector(con_sdm, ply_pa, "ply_planareas")
  dbExecute(con_sdm, "CREATE INDEX idx_ply_planareas_geom ON ply_planareas USING RTREE (geom)") # TODO: redo with USING RTREE
}

dbExecute(con_sdm, "DROP TABLE ply_planareas")
dbListTables(con_sdm)
ply_pa <- ddbs_read_vector(con_sdm, "ply_planareas")
```

## Create 0.05° Grid Cells

```{r create_grid}
#| message: false

cell_tif <- glue("{dir_data}/derived/r_bio-oracle_planarea.tif")

ply_pa_buf <- ply_pa |> 
  st_buffer(10*1000) |>    # 10 km 
  # st_wrap_dateline()     # [   0, 360] -> [-180, 180]
  st_shift_longitude()     # [-180, 180] -> [    0,360]
  
r_bo   <- get_bo_raster() # Bio-Oracle

r_cell <- r_bo |>                    
  rotate(left=F) |>                # 3600 x 7200 = 25,920,000
  mask(ply_pa_buf, touches = F) |> # 2006 x 3103 =  6,224,618
  trim()
r_cell$area_km2  <- terra::cellSize(r_cell$depth_mean, mask = T, unit = "km")
r_cell$width_km2 <- sqrt(r_cell$area_km2)
# minmax(r_cell$width_km2)
# min  2.007380
# max  5.547313
r_cell$cell_id   <- 1:ncell(r_cell) # assign cell IDs
r_cell <- r_cell |> 
  subset(c(
    "cell_id",
    setdiff(names(r_cell), "cell_id") ))
r_cell <- mask(r_cell, r_cell$depth_mean)

pt_cell <- terra::as.data.frame(r_cell, cells = F, xy = T, na.rm = F) |> 
  filter(!is.na(depth_mean)) |>
  rename(
    lon_360 = x,
    lat     = y) |> 
  mutate(
    cell_id = as.integer(cell_id),
    lon     = ifelse(lon_360 > 180, lon_360 - 360, lon_360) ) |>
  relocate(lon, .before = lon_360) |>
  tibble() |> # nrow: 661,372
  st_as_sf(
    coords = c("lon", "lat"),
    crs    = "EPSG:4326",
    remove = F)
st_geometry(pt_cell) <- "geom"

# dbExecute(con_sdm, "DROP TABLE cell")
ddbs_write_vector(con_sdm, pt_cell, "cell")

# Add primary key constraint to existing column
dbExecute(con_sdm, "
  ALTER TABLE cell ADD CONSTRAINT pk_constraint PRIMARY KEY (cell_id)")
# Add index to geometry column
dbExecute(con_sdm, "
  CREATE INDEX idx_cell_geom ON cell USING RTREE (geom)") # TODO: redo with USING RTREE

# save the planarea raster template for later use
writeRaster(
  r_cell,
  cell_tif,
  filetype = "COG",
  overwrite = T,
  verbose   = T)
```

```{r}
#| label: post_cell_tables
#| eval: false

# dbExecute(con_sdm, "DROP TABLE model_cell")
dbExecute(con_sdm, "
  CREATE TABLE model_cell (
    mdl_seq INTEGER REFERENCES model(mdl_seq),
    cell_id INTEGER REFERENCES cell(cell_id),
    value DOUBLE,
    PRIMARY KEY (cell_id, mdl_seq) )")

# dbExecute(con_sdm, "DROP TABLE cell_metric")
dbExecute(con_sdm, "
  CREATE TABLE cell_metric (
    cell_id INTEGER REFERENCES cell(cell_id),
    metric_seq INTEGER REFERENCES metric(metric_seq),
    value DOUBLE,
    PRIMARY KEY (cell_id, metric_seq) )")

# dbExecute(con_sdm, "DROP TABLE region_cell")
# dbExecute(con_sdm, "
#   CREATE TABLE region_cell (
#     rgn_seq INTEGER REFERENCES region(rgn_seq),
#     cell_id INTEGER REFERENCES cell(cell_id),
#     pct_covered INTEGER,
#     PRIMARY KEY (rgn_seq, cell_id) )")
dbExecute(con_sdm, "
  CREATE TABLE zone_cell (
    zone_seq INTEGER, -- REFERENCES region(rgn_seq),
    cell_id INTEGER,  -- REFERENCES cell(cell_id),
    pct_covered INTEGER,
    PRIMARY KEY (zone_seq, cell_id) )")

# dbExecute(con_sdm, "DROP INDEX idx_model_cell_cell_id")
# dbExecute(con_sdm, "DROP INDEX idx_model_cell_mdl_seq")
# dbExecute(con_sdm, "CREATE INDEX idx_region_cell_rgn ON region_cell(rgn_seq)")
# dbExecute(con_sdm, "CREATE INDEX idx_region_cell_cell ON region_cell(cell_id)")
dbExecute(con_sdm, "CREATE INDEX idx_zone_cell_rgn ON zone_cell(zone_seq)")
dbExecute(con_sdm, "CREATE INDEX idx_zone_cell_cell ON zone_cell(cell_id)")
```


## Process Aquamaps downsized models

```{r process_species}
#| eval: false

# this chunk assumes the iterate_species_in_planareas chunk from 
# ingest_aquamaps_res05.qmd has been run and produced species rasters
# Doh! The first one Chn-013ca97e-46a8-4c5e-b398-8bb4f340f88b.tif shows values > 100, and whack compared to
#  https://aquamaps.org/preMap2.php?cache=1&SpecID=Chn-013ca97e-46a8-4c5e-b398-8bb4f340f88b, 
# so retrying just for PA extent and directly starting with AquaMaps normal resolution

cell_tif   <- glue("{dir_data}/derived/r_bio-oracle_planarea.tif")
spp_pa_csv <- glue("{dir_data}/derived/aquamaps.org/spp_boem-planareas-2025.csv")

spp_keys_done <- tbl(con_sdm, "model") |> 
  filter(ds_key == "am_0.05") |> 
  pull(taxa)
stopifnot(sum(duplicated(spp_keys_done)) == 0)

d_spp_pa <- read_csv(spp_pa_csv) |>  # 17,550 x 2
  arrange(desc(n_cells_pa), sp_key) |> 
  filter(!sp_key %in% spp_keys_done)

if (!all(sapply(c("r_hcaf", "r_cell"), exists))){
  r_hcaf  <- get_hcaf_raster()
  r_cell  <- rast(cell_tif)
}

# dbExecute(con_sdm, glue("TRUNCATE model"))
# dbExecute(con_sdm, glue("TRUNCATE model_cell"))
# dbExecute(con_sdm, glue("TRUNCATE species"))

# INFO [2025-05-28 04:28:49] Processing 1/17,540: Fis-29877
# INFO [2025-05-28 04:33:48] Processing 85/17,540: Fis-25165
# 17540 / (85/5) / 60 = 17 hrs
# INFO [2025-05-28 06:30:59] Processing 1,175/17,540: Fis-34595
# 17540 / (1175/122) / 60 = 30 hrs
# redo with fast processing on raster[idx] <- val; terra::rotate() upgrade; no indexes on model_cell
# INFO [2025-05-28 15:11:33] Processing 1/17,549: ITS-Mam-180530
# INFO [2025-05-28 15:12:33] Processing 37/17,549: Fis-162326
# 17549 / 37 / 60 = 7.9 hrs
# INFO [2025-05-28 15:13:33] Processing 71/17,549: Fis-23454
# 17549 / (71/2) / 60 = 8.2 hrs
# INFO [2025-05-28 15:16:33] Processing 178/17,549: ITS-Mam-180417
# 17549 / (178/5) / 60 = 8.2 hrs
# INFO [2025-05-28 15:24:33] Processing 471/17,549: Fis-21671
# 17549 / (471/13) / 60 = 8.1 hrs
# INFO [2025-05-28 16:38:34] Processing 3,355/17,549: Fis-163781
# 17549 / (3355/130) / 60 = 11.3 hrs
# INFO [2025-05-28 18:26:33] Processing 8,016/17,549: W-Bry-111513
# 17549 / (8016/195) / 60 = 7.1 hrs
# INFO [2025-05-28 18:31:44] Processing 8,239/17,549: Fis-28234

n <- nrow(d_spp_pa)
tic <- Sys.time()
for (i in 1:n){  # i = 10 # i = 1175
# for (i in 1:10){  # i = 2
  sp_key     <- d_spp_pa$sp_key[i]      # sp_key = "ITS-Mam-180528" # sp_key = "Fis-34595"
  n_cells_pa <- d_spp_pa$n_cells_pa[i]

  toc <- Sys.time()
  dt <- difftime(toc, tic, units = "mins") # |> as.numeric()
  eta <- tic + (dt / i) * n
  
  log_info(
    "Processing {format(i, big.mark=',')}/{format(nrow(d_spp_pa), big.mark=',')}: {sp_key} ~ ETA: {eta}")

  # source(here("libs/am_functions.R"))
  # source(here("libs/sdm_functions.R"))
  r_am <- am_sp_rast(sp_key, con_am, r_hcaf) |> 
    rotate()  # [rotate() shifts cells of non-global spatRast](https://github.com/rspatial/terra/issues/1831)
  r_ds <- downsample_sp_rast(r_am, r_cell$depth_mean)

  # get species info from original database
  sp_info <- tbl(con_am, "spp") |>
    filter(sp_key == !!sp_key) |>
    collect()
  
  # delete existing model
  mdl_seqs <- tbl(con_sdm, "model") |> 
    filter(ds_key == "am_0.05", taxa == !!sp_key) |> 
    pull(mdl_seq)
  if (length(mdl_seqs > 0)) {
    dbExecute(con_sdm, glue("DELETE FROM model WHERE ds_key = 'am_0.05' AND taxa = '{sp_key}'"))
    dbExecute(con_sdm, glue("DELETE FROM model_cell WHERE mdl_seq IN ({paste(mdl_seqs, collapse = ',')})"))
    dbExecute(con_sdm, glue("DELETE FROM species WHERE ds_key = 'am_0.05' AND taxa = '{sp_key}'"))
  }
  
  # create model for this species (one model per species for AquaMaps)
  d_model <- tibble(
    ds_key      = "am_0.05",
    taxa        = sp_key,  # for AquaMaps, taxa equals sp_key
    time_period = "2019",
    region      = "Global",
    mdl_type    = "suitability",
    description = glue(
      "AquaMaps suitability for {sp_info$genus[1]} {sp_info$species[1]}, interpolated to 0.05° resolution") )
  dbWriteTable(con_sdm, "model", d_model, append = TRUE)
  
  # get the mdl_seq that was just created
  mdl_seq <- dbGetQuery(con_sdm, glue("
    SELECT mdl_seq FROM model 
    WHERE ds_key = '{d_model$ds_key}' 
      AND taxa = '{d_model$taxa}'
    ORDER BY mdl_seq DESC LIMIT 1 "))$mdl_seq
  
  # determine species category
  sp_cat <- case_when(
    sp_info$class[1] == "Actinopterygii" ~ "fish",
    sp_info$class[1] == "Chondrichthyes" ~ "fish",
    sp_info$class[1] == "Mammalia" ~ "marine mammal",
    sp_info$class[1] == "Reptilia" ~ "sea turtle",
    sp_info$phylum[1] == "Mollusca" ~ "mollusk",
    # sp_info$phylum[1] == "Arthropoda" && sp_info$subphylum[1] == "Crustacea" ~ "crustacean",
    sp_info$phylum[1] == "Arthropoda" ~ "crustacean",
    TRUE ~ "other")
  
  # add species to database
  d_species <- tibble(
    ds_key          = "am_0.05",
    taxa            = sp_key,
    sp_key          = sp_key,
    aphia_id        = NA_integer_,  # TODO: lookup from WoRMS
    gbif_id         = NA_integer_,  # TODO: lookup from GBIF
    itis_id         = NA_integer_,  # TODO: lookup from ITIS
    scientific_name = glue("{sp_info$genus[1]} {sp_info$species[1]}"),
    common_name     = sp_info$common_name[1],
    sp_cat          = sp_cat)
  dbWriteTable(con_sdm, "species", d_species, append = T)

  d_mdl_cell <- as.data.frame(r_ds, cells = T, na.rm = T) |> 
    tibble() |> 
    mutate(
      mdl_seq = mdl_seq,
      value   = as.integer(suitability) ) |> 
    select(cell_id = cell, mdl_seq, value) |> 
    arrange(cell_id)
  dbWriteTable(con_sdm, "model_cell", d_mdl_cell, append = T)
}
```

## Add model_cell indexes after mass insert

```{r add_indexes}
dbExecute(con_sdm, "CREATE INDEX idx_species_taxa ON species(ds_key, taxa)")
dbExecute(con_sdm, "CREATE INDEX idx_model_taxa ON model(ds_key, taxa)")
dbExecute(con_sdm, "CREATE INDEX idx_model_cell_mdl_seq ON model_cell(mdl_seq)")
# dbExecute(con_sdm, "CREATE INDEX idx_model_cell_cell_id ON model_cell(cell_id)")
# Out of Memory Error: could not allocate block of size 256.0 KiB (9.3 GiB/9.3 GiB used)
# TODO: order by cell_id in first column and save as parquet
```

## Calculate Biodiversity Metrics

```{r calc_metrics}
#| eval: false

# first create metric definitions
metrics_to_create <- tibble::tribble(
  ~metric_type,         ~description,
  "species_richness",   "Number of species with suitability > 0.5",
  "shannon_index",      "Shannon diversity index based on suitability values",
  "simpson_index",      "Simpson diversity index based on suitability values"
)

for (i in 1:nrow(metrics_to_create)) {
  dbExecute(con_sdm, glue("
    INSERT INTO metric (metric_type, description)
    VALUES ('{metrics_to_create$metric_type[i]}', '{metrics_to_create$description[i]}')
  "))
}

# get metric IDs
metric_ids <- dbGetQuery(con_sdm, "SELECT metric_seq, metric_type FROM metric")

# calculate species richness per cell
message("Calculating species richness...")

richness_metric_seq <- metric_ids$metric_seq[metric_ids$metric_type == "species_richness"]

# dbExecute(con_sdm, "TRUNCATE cell_metric")

# dbExecute(con_sdm, glue("DELETE FROM cell_metric WHERE metric_seq = {richness_metric_seq}"))

system.time({
  dbExecute(con_sdm, glue("
  INSERT INTO cell_metric (cell_id, metric_seq, value)
  SELECT 
    mc.cell_id,
    {richness_metric_seq} as metric_seq,
    COUNT(DISTINCT m.taxa) as value
  FROM model_cell mc
  JOIN model m ON mc.mdl_seq = m.mdl_seq
  WHERE mc.value > 0.5  -- presence threshold
  GROUP BY mc.cell_id"))
}) # 34.346 sec

# calculate Shannon diversity index
message("Calculating Shannon diversity...")

shannon_metric_seq <- metric_ids$metric_seq[metric_ids$metric_type == "shannon_index"]

dbExecute(con_sdm, glue("
INSERT INTO cell_metric (cell_id, metric_seq, value)
SELECT 
  cell_id,
  {shannon_metric_seq} as metric_seq,
  -SUM(p * LN(p)) as value
FROM (
  SELECT 
    mc.cell_id,
    mc.value / SUM(mc.value) OVER (PARTITION BY mc.cell_id) as p
  FROM model_cell mc
  WHERE mc.value > 0
) t
WHERE p > 0
GROUP BY cell_id
"))
```

## Calculate Extinction Risk Metrics by Class

```{r calc_metrics_redlist}
#| eval: false

# tbl(con_sdm, "species") |> 
#   pull(sp_cat) |> 
#   table(useNA = "ifany")
#  crustacean           fish marine mammal        mollusk          other    sea turtle 
#       3,025          6,328            85          3,920          4,159            33  
sp_cats <- c(
  "all",  # 17,550
  tbl(con_sdm, "species") |> distinct(sp_cat) |> pull(sp_cat))
for (sp_cat in sp_cats){ # sp_cat = sp_cats[1]
  
  message(glue("Calculating extinction risk metrics for {sp_cat}..."))
  
  metric_type <- glue("extrisk_{str_replace(sp_cat, ' ', '_')}")
  description <- glue("Extinction risk for {sp_cat}")
  # TODO: rename metric_type -> metric_key, description

  dbExecute(con_sdm, glue("
    DELETE FROM metric WHERE metric_type = '{metric_type}'" ))
    
  dbExecute(con_sdm, glue("
    INSERT INTO metric (metric_type, description)
    VALUES ('{metric_type}', '{description}')" ))

  metric_seq <- dbGetQuery(con_sdm, glue(
    "SELECT max(metric_seq) AS metric_seq FROM metric WHERE metric_type = '{metric_type}'"))
  
  d_rl <- tbl(con_sdm, "species") |> # 17,550
    filter(if (!!sp_cat == "all") T else sp_cat == !!sp_cat) |> 
    select(taxa, sp_key, redlist_code) |> 
    filter(
      !is.na(redlist_code),
      !redlist_code %in% c("DD","EX")) |> 
    collect() |> 
    mutate(
      rl_score = case_match(
        redlist_code,
        # skipped: EX (extinct), NE (not evaluated), DD (data deficient), <NA> (not available)
        # https://oceanhealthindex.org/images/htmls/Supplement.html#62_Biodiversity
        "CR" ~ 1,       #  - CR: Critically Endangered	
        "EN" ~ 0.8,     #  - EN: Endangered	
        "VU" ~ 0.6,     #  - VU: Vulnerable
        "NT" ~ 0.4,     #  - NT: Near Threatened
        "LC" ~ 0.2))    #  - LC: Least Concern
  
  # dbListTables(con_sdm)
  duckdb_register(con_sdm, "temp_rl", d_rl)
  dbExecute(con_sdm, glue("
    INSERT INTO cell_metric (cell_id, metric_seq, value)
    SELECT 
      mc.cell_id,
      {metric_seq} as metric_seq,
      SUM(rl.rl_score * mc.value)
    FROM model_cell mc
    JOIN model m ON mc.mdl_seq = m.mdl_seq
    JOIN temp_rl rl ON m.taxa = rl.taxa
    GROUP BY mc.cell_id"))  # 662,075
  duckdb::duckdb_unregister(con_sdm, "temp_rl")
}

```

## Create new EcoRegions for 2025 Planning Areas

```{r calc_metrics_planning_areas}
librarian::shelf(
  mapview, units)
source(here("libs/db.R")) # con

cell_tif <- glue("{dir_data}/derived/r_bio-oracle_planarea.tif")
pa_geo   <- glue("{dir_data}/derived/pa_oa_2025.geojson") # created in: ingest_productivity.qmd
er_geo   <- glue("{dir_data}/derived/er_oa_2025.geojson") # new

set_geom_ctr_area <- function(x){
  x |>
    st_set_geometry("geom") |>
    mutate(
      ctr      = st_centroid(geom),
      ctr_lon  = ctr |> st_coordinates() %>% .[,"X"],
      ctr_lat  = ctr |> st_coordinates() %>% .[,"Y"],
      area_km2 = st_area(geom) |>
        set_units(km^2) |>
        as.numeric() ) |>
    select(-ctr)
}

drop_ctr_area <- function(x){
  x |>
    select(-ctr_lon, -ctr_lat, -area_km2)
}

ply_pa <- st_read(pa_geo) |>                # [    0,360]
  st_wrap_dateline()   # [   0, 360] -> [-180, 180]
  # st_shift_longitude() # [-180, 180] -> [    0,360]
ply_er <- msens::ply_ecorgns |>         # [-180, 180]
  # st_shift_longitude() |>               # [    0,360]
  # match names to https://tile.marinesensitivity.org/public.ply_planareas_2025.html
  rename(
    region_key     = boemrgn_key,
    region_name    = boemrgn_name,
    ecoregion_key  = ecorgn_key,
    ecoregion_name = ecorgn_name) |> 
  mutate(
    ecoregion_key = str_replace(ecoregion_key, "er", ""),
    region_key    = str_replace(region_key,    "br", ""),
    region_name   = case_match(
      region_name,
      "Gulf of Mexico" ~ "Gulf of America",
      .default = region_name),
    region_key   = case_match(
      region_key,
      "GOM" ~ "GA",
      "PAC" ~ "PA",
      "ATL" ~ "AT",
      .default = region_key),
    ecoregion_name   = case_match(
      ecoregion_name,
      "Eastern GOM"             ~ "Eastern Gulf of America",
      "Western and Central GOM" ~ "Western and Central Gulf of America",
      .default = ecoregion_name),
    ecoregion_key = case_match(
      ecoregion_key,
      "EGOM"  ~ "EGOA",
      "WCGOM" ~ "WCGOA",
      .default = ecoregion_key) ) |> 
  filter(
    !(region_key == "ATL" & ecoregion_key == "EGOA"))
  
mapView(ply_pa, zcol = "planarea_key") +
  mapView(ply_er, zcol = "ecoregion_key")

lst_ernew <- list(
  "HAR" = "HAR",
  "PUR" = "PUR",
  "PIS" = c("AMS","GUA","HAW","HOW","JAR","JOH","PAL","WAK"))
ernew <- lst_ernew |> unlist() |> unname()
d_ernew <- lst_ernew |> 
  enframe(
    name  = "ecoregion_key",
    value = "planarea_key") |> 
  unnest(planarea_key) |> 
  mutate(
    ecoregion_name = case_match(
      ecoregion_key,
      "HAR" ~ "High Arctic",
      "PUR" ~ "Puerto Rico and U.S. Virgin Islands",
      "PIS" ~ "Pacific Island Territories" ) )

ply_er_new <- ply_pa |> 
  filter(
    planarea_key %in% ernew) |> 
  left_join(
    d_ernew,
    by = "planarea_key") |> 
  select(region_key, region_name, ecoregion_key, ecoregion_name)

ply_pa_haser <- ply_pa |> 
  filter(
    !planarea_key %in% ernew) |> 
    drop_ctr_area()
mapView(ply_pa_haser)

x <- st_intersection(
  ply_pa_haser,
  ply_er |> 
    drop_ctr_area()) |> 
  st_make_valid()

x_u <- x |> 
  st_union() |> 
  st_make_valid()

sf_use_s2(F)
y <- st_difference(
  ply_pa_haser,
  x_u) |> 
  st_make_valid() |> 
  set_geom_ctr_area() |> 
  filter(area_km2 > 2)
sf_use_s2(T)

# st_shift_longitude() # |> 
# write_sf(
#   glue("{dir_data}/derived/er_pa2025-slivers.geojson"))

yp <- y |> 
  drop_ctr_area() |> 
  st_cast("POLYGON") |> 
  st_make_valid() |> 
  set_geom_ctr_area() |> 
  filter(area_km2 > 2) |> 
  rowid_to_column()

mapview(x, zcol = "ecoregion_key") +
  mapview(yp, zcol = "rowid")

ypi <- yp |> 
  mutate(
    ecoregion_key = case_match(
      rowid,
      1 ~ "SECS",
      2 ~ "NECS",
      3 ~ "NECS",
      4 ~ "NECS",
      5 ~ "CAC") ) |> 
  select(ecoregion_key) |> 
  left_join(
    ply_er |> 
      st_drop_geometry() |> 
      select(region_key, region_name, ecoregion_key, ecoregion_name),
    by = "ecoregion_key") 

ply_er_new <- bind_rows(
  x |> 
    select(region_key, region_name, ecoregion_key, ecoregion_name),
  ply_er_new,
  ypi) |> 
  group_by(region_key, region_name, ecoregion_key, ecoregion_name) |> 
  summarize(.groups = "drop") |> 
  st_shift_longitude()

mapView(ply_er_new, zcol = "ecoregion_key")

st_write(ply_er_new, er_geo)

ply_er <- st_read(er_geo) |> 
  st_wrap_dateline()    # [   0, 360] -> [-180, 180]
  # st_shift_longitude()  # [-180, 180] -> [    0,360]
st_geometry(ply_er) = "geom" # rename  

mapView(ply_er)

tbl_er <- "ply_ecoregions_2025"
if (!tbl_er %in% dbListTables(con)){
  st_write(ply_er, con, tbl_er, delete_layer = T)
  
  # enforce SRID so shows up in tile.marinesensivity.org
  dbExecute(con, glue(
    "SELECT UpdateGeometrySRID('public','{tbl_er}','geom',4326);"))
}
```

## Calculate Extinction Risk by Planning Area, EcoRegion

```{r zone_stats}
cell_tif <- glue("{dir_data}/derived/r_bio-oracle_planarea.tif")
pa_geo   <- glue("{dir_data}/derived/pa_oa_2025.geojson") # created in: ingest_productivity.qmd
er_geo   <- glue("{dir_data}/derived/er_oa_2025.geojson") # created above

r_cell <- rast(cell_tif)
ply_er <- st_read(er_geo)
ply_pa <- st_read(pa_geo)

# get percent each feature covers each cell
r_pa <- rasterize(
  ply_pa, 
  r_cell, 
  fun     = "max", 
  touches = T, 
  cover   = T, 
  by      = "planarea_key")
r_er <- rasterize(
  ply_er, 
  r_cell, 
  fun     = "max", 
  touches = T, 
  cover   = T, 
  by      = "ecoregion_key")
unique(names(r_er))

# convert to data frame
d_pa <- terra::as.data.frame(
  r_pa, cells = T, na.rm = F, wide = F) |> 
  filter(
    !is.na(values)) |>  # 634,417 × 3
  tibble()
d_er <- terra::as.data.frame(
  r_er, cells = T, na.rm = F, wide = F) |> 
  filter(
    !is.na(values)) |>  # 380,875 × 3
  tibble()

register_zones <- function(ply, tbl, fld) {
  # register zones in the database
  dbExecute(con_sdm, glue("DELETE FROM zone WHERE tbl = '{tbl}' AND fld = '{fld}'"))
  for (i in 1:nrow(ply)) { # i = 1
    dbExecute(con_sdm, glue("
      INSERT INTO zone (tbl, fld, value)
      VALUES ('{tbl}', '{fld}', '{ply[[fld]][i]}') " ) )
  }
}

# tbl(con_sdm, "zone") |> 
#   group_by(tbl, fld) |> 
#   summarize(n = n())

insert_zone_data <- function(df, tbl, fld) {
  # df = d_pa; tbl = "ply_planareas_2025"; fld = "planarea_key"
  
  # get zone and metric IDs
  d_zone <- dbGetQuery(con_sdm, glue("
    SELECT zone_seq, value FROM zone WHERE tbl = '{tbl}' AND fld = '{fld}'" ))
  
  d_zone_cell <- df |> 
    left_join(
      d_zone,
      by = c("layer" = "value")) |> 
    mutate(
      zone_seq    = as.integer(zone_seq),
      pct_covered = as.integer(round(values * 100)) ) |> 
    filter(pct_covered > 0) |> 
    select(
      zone_seq, 
      cell_id = cell, 
      pct_covered)
  
  dbExecute(con_sdm, glue("DELETE FROM zone_cell WHERE zone_seq IN ({paste(d_zone$zone_seq, collapse = ',')})"))
  dbWriteTable(con_sdm, "zone_cell", d_zone_cell, append = T)
}

# register zones and insert zone data
dbExecute(con_sdm, "TRUNCATE zone_cell")
register_zones(ply_pa, "ply_planareas_2025", "planarea_key")
insert_zone_data(d_pa, "ply_planareas_2025", "planarea_key")

register_zones(ply_er, "ply_ecoregions_2025", "ecoregion_key")
insert_zone_data(d_er, "ply_ecoregions_2025", "ecoregion_key")
```

## Create min|max ExtinctionRisk per SpeciesCategory and Ecoregion

```{r calc_ecoregion_minmax}
#| eval: false

# create metric per sp_cat and min|max
tbl(con_sdm, "metric")

sp_cats <- c(
  "all",
  tbl(con_sdm, "species") |> distinct(sp_cat) |> pull(sp_cat))

for (sp_cat in sp_cats){ # sp_cat = sp_cats[1]
  for (fxn in c("min","max")){ # fxn = "min"
    
    m_type <- glue("ecoregion_extrisk_{fxn}_{str_replace(sp_cat, ' ', '_')}")
    
    dbExecute(con_sdm, glue(
      "DELETE FROM metric WHERE metric_type = '{m_type}'"))
    
    dbExecute(con_sdm, glue(
      "INSERT INTO metric (metric_type, description)
      VALUES ('{m_type}', 'Ecoregion {fxn} extinction risk for {sp_cat}')" ) )
    
    # get metric ID
    m_seq <- dbGetQuery(con_sdm, glue(
      "SELECT metric_seq FROM metric WHERE metric_type = '{m_type}'"))$metric_seq
      
    # calculate min/max extinction risk per ecoregion
    dbExecute(con_sdm, glue(
      "DELETE FROM zone_metric WHERE metric_seq = {m_seq}"))
    dbExecute(con_sdm, glue(
      "INSERT INTO zone_metric (zone_seq, metric_seq, value)
      SELECT 
        z.zone_seq,
        {m_seq} as metric_seq,
        {fxn}(cm.value) as value
      FROM zone z
      LEFT JOIN zone_cell zc ON z.zone_seq = zc.zone_seq
      LEFT JOIN cell_metric cm ON zc.cell_id = cm.cell_id
      WHERE
        z.fld = 'ecoregion_key'
        AND cm.metric_seq IN (
          SELECT metric_seq FROM metric WHERE metric_type = 'extrisk_{str_replace(sp_cat, ' ', '_')}')
      GROUP BY z.zone_seq" ))
  } }

tbl(con_sdm, "zone")

```

## Generate Biodiversity Raster

```{r generate_raster}
#| eval: false

# load global raster template
# r_global <- readRDS(glue("{dir_data}/derived/r_global_0.05.rds"))
cell_tif <- glue("{dir_data}/derived/r_bio-oracle_planarea.tif")

r_cell <- rast(cell_tif)

# get species richness metric ID
richness_metric_seq <- dbGetQuery(con_sdm, "
  SELECT metric_seq FROM metric WHERE metric_type = 'species_richness'")$metric_seq

# query species richness for a region
richness_df <- dbGetQuery(con_sdm, glue("
  SELECT 
    cm.cell_id,
    cm.value as species_richness
  FROM cell_metric cm
  WHERE cm.metric_seq = {richness_metric_seq}
    AND cm.cell_id IN (
      SELECT cell_id FROM cell 
      WHERE lon BETWEEN -180 AND -60  -- limit to a region for testing
        AND lat BETWEEN 20 AND 50)")) |> 
  tibble()

# convert to raster using the new function
r_richness <- create_raster_from_df(
  richness_df, 
  r_template = r_cell,
  value_col = "species_richness") # plot(r_richness)

# display with mapgl
mapgl_rast <- function(r, palette = "viridis", layer_name = "SDM") {
  # convert raster to data frame
  r_df <- as.data.frame(r, xy = TRUE) |>
    rename(value = 3)
  
  # create mapgl visualization
  mapgl() |>
    add_raster_tile_layer(
      id = "sdm_raster",
      source = r,
      source_layer = layer_name,
      color_palette = palette
    )
}

# display with mapgl
# TODO:
mapgl_rast2<- function(r, palette = "viridis", layer_name = "SDM") {
  
  # r <- r_pa[["depth_mean"]]
  # r <- r_ds
  r <- r_richness
  # plot(r_am)
  # plot(r_ds)
  
  rng_r <- minmax(r) |> as.numeric()
  cols_r <- rev(RColorBrewer::brewer.pal(11, "Spectral"))
  
  mapboxgl(
    style = mapbox_style("dark"),
    zoom = 2,
    center = c(-141.3, 13.4)) |>  # ply_pa centroid
    add_image_source(
      id     = "r_src",
      data   = r,
      colors = cols_r) |> 
    add_raster_layer(
      id                = "r_lyr",
      source            = "r_src",
      raster_resampling = "nearest") |> 
    mapgl::add_legend(
      "Colorscale",
      values = rng_r,
      colors = cols_r,
      position = "bottom-right") |> 
    add_fullscreen_control(position = "top-left") |>
    add_navigation_control() |> 
    add_scale_control()
}

# visualize
mapgl_rast(r_richness, palette = "YlOrRd", layer_name = "Species Richness")
```

## Summarize by Planning Area

```{r summarize_planning_areas}
#| eval: false

# first, register planning areas as regions
planning_area_regions <- dbGetQuery(con_sdm, "
  SELECT pa_key, pa_name FROM planning_areas")

for (i in 1:nrow(planning_area_regions)) {
  dbExecute(con_sdm, glue("
    INSERT INTO region (tbl_col, value, col_geom)
    VALUES ('planning_areas.pa_key', '{planning_area_regions$pa_key[i]}', 'geom')
  "))
}

# get region and metric IDs
region_ids <- dbGetQuery(con_sdm, "
  SELECT rgn_seq, value FROM region WHERE tbl_col = 'planning_areas.pa_key'
")

richness_metric_seq <- dbGetQuery(con_sdm, "
  SELECT metric_seq FROM metric WHERE metric_type = 'species_richness'
")$metric_seq

# calculate average species richness per planning area
message("Calculating planning area metrics...")

for (i in 1:nrow(region_ids)) {
  pa_key <- region_ids$value[i]
  rgn_seq <- region_ids$rgn_seq[i]
  
  # calculate metric for this region
  result <- dbGetQuery(con_sdm, glue("
    SELECT AVG(cm.value) as avg_value
    FROM planning_areas p
    JOIN cell c ON ST_Within(c.geom, p.geom)
    JOIN cell_metric cm ON c.cell_id = cm.cell_id
    WHERE p.pa_key = '{pa_key}'
      AND cm.metric_seq = {richness_metric_seq}
  "))
  
  if (!is.na(result$avg_value[1])) {
    dbExecute(con_sdm, glue("
      INSERT INTO region_metric (rgn_seq, metric_seq, value)
      VALUES ({rgn_seq}, {richness_metric_seq}, {result$avg_value[1]})
    "))
  }
}

# query results
pa_metrics <- dbGetQuery(con_sdm, "
SELECT 
  p.pa_key,
  p.pa_name,
  rm.value as avg_richness
FROM region r
JOIN region_metric rm ON r.rgn_seq = rm.rgn_seq
JOIN planning_areas p ON r.value = p.pa_key
WHERE r.tbl_col = 'planning_areas.pa_key'
  AND rm.metric_seq = (SELECT metric_seq FROM metric WHERE metric_type = 'species_richness')
ORDER BY rm.value DESC
")

print(pa_metrics)
```

## Add Environmental Layers

```{r add_env_layers}
#| eval: false

# example: add primary productivity from ingest_productivity.qmd
npp_tif <- here("data/derived/vgpm-viirs_2023-daily-avg.tif")

if (file.exists(npp_tif)) {
  r_npp <- rast(npp_tif)
  
  # resample to 0.05 degree resolution
  r_template <- rast(
    extent = ext(-180, 180, -90, 90),
    resolution = 0.05,
    crs = "EPSG:4326"
  )
  
  r_npp_res <- resample(r_npp, r_template, method = "bilinear")
  
  # extract values
  npp_df <- as.data.frame(r_npp_res, xy = TRUE) |>
    filter(!is.na(npp_daily_avg)) |>
    mutate(
      cell_lon = round(x / 0.05) * 0.05,
      cell_lat = round(y / 0.05) * 0.05
    )
  
  # insert into env_layers
  # (similar process as species values)
}
```

## Export to Parquet

```{r ck_model_cell}

d <- dbGetQuery(con_sdm, "SELECT * FROM duckdb_tables() WHERE table_name = 'model_cell';") 
glimpse(d) # 604,449,630 rows
# $ database_name          <chr> "sdm"
# $ database_oid           <dbl> 546
# $ schema_name            <chr> "main"
# $ schema_oid             <dbl> 548
# $ table_name             <chr> "model_cell"
# $ table_oid              <dbl> 605
# $ comment                <chr> NA
# $ tags                   <list> [<data.frame[0 x 2]>]
# $ internal               <lgl> FALSE
# $ temporary              <lgl> FALSE
# $ has_primary_key        <lgl> FALSE
# $ estimated_size         <dbl> 604449630
# $ column_count           <dbl> 3
# $ index_count            <dbl> 1
# $ check_constraint_count <dbl> 0
# $ sql                    <chr> "CREATE TABLE model_cell(cell_id INTEGER, mdl_seq INTEGER, \"value\" INTEGER…


tbl(con_sdm, "model_cell")
#    cell_id mdl_seq value
#      <int>   <int> <int>
#  1     990       1     1
#  2     991       1     1
#  3     992       1     1
```


```{r export_parquet}
#| eval: false
# TODO: model_cell: PARTITION BY (mdl_seq);") # https://duckdb.org/docs/stable/data/parquet/overview

sdm_db <- glue("{dir_data}/derived/sdm.duckdb")

# source(here("libs/sdm_functions.R"))
con_sdm <- dbConnect(duckdb(dbdir = sdm_db, read_only = F)) |> 
  load_duckdb_extensions()
 
# dbListTables(con_sdm)
dbExecute(con_sdm, "SET threads = 8;")
dbExecute(con_sdm, "SET memory_limit = '20GB';")

dir_pq <- glue("{dir_data}/derived/sdm_parquet")
dir_create(dir_pq)

dbExecute(con_sdm, glue(
  "EXPORT DATABASE '{normalizePath(dir_pq)}' (
    FORMAT parquet,
    COMPRESSION zstd,
    ROW_GROUP_SIZE 100_000, 
    ROW_GROUPS_PER_FILE 1000,
    OVERWRITE );")) # 3 secs

# mdl_cell_pq <- glue("{dir_pq}/model_cell.parquet")
# mdl_cell_pq <- glue("{dir_data}/derived/sdm_model_cell.parquet")

# Write sorted parquet -- note that this is not a partitioned table, not faster than native
# dbExecute(con_sdm, glue(
#   "COPY (
#     SELECT cell_id, mdl_seq, value 
#     FROM model_cell 
#     ORDER BY cell_id, mdl_seq ) TO '{mdl_cell_pq}' 
#   (FORMAT 'parquet', COMPRESSION 'zstd') "))
# 
# # Read back as external table or view
# dbExecute(con_sdm, glue(
#   "CREATE VIEW model_cell_view AS SELECT * FROM '{mdl_cell_pq}' " ))
# dbExecute(con_sdm, "DROP VIEW IF EXISTS model_cell_view;")
# TODO: use VIEWs for read-only duckdb later for chunking out files, 
#   but doesn't seem faster than native duckdb table

```

## Import from Parquet

```{r import_parquet}

dir_pq <- glue("{dir_data}/derived/sdm_parquet")
sdm_dd <- glue("{dir_data}/derived/sdm.duckdb")

if (exists("con_sdm")){
  dbDisconnect(con_sdm, shutdown = T)
  rm(con_sdm)
}
duckdb_shutdown(duckdb())

if (file_exists(sdm_dd))
  file_delete(sdm_dd)

# source(here("libs/sdm_functions.R"))
con_sdm <- dbConnect(duckdb(dbdir = sdm_dd, read_only = F)) |> 
  load_duckdb_extensions()
 
# dbListTables(con_sdm)
dbExecute(con_sdm, "SET threads = 8;")
dbExecute(con_sdm, "SET memory_limit = '20GB';")

# dbExecute(con_sdm, glue("IMPORT DATABASE '{normalizePath(dir_pq)}'"))
# dbListTables(con_sdm)

for (pq in dir_ls(dir_pq, glob = "*.parquet")[-1]) { # pq = dir_ls(dir_pq, glob = "*.parquet")[1]
  tbl <- path_file(pq) |> str_remove("\\.parquet$")
  if (!dbExistsTable(con_sdm, tbl)) {
    message(glue("Importing {tbl} from {basename(pq)}/*.parquet"))
    dbExecute(con_sdm, glue("CREATE TABLE {tbl} AS SELECT * FROM '{pq}/*.parquet'"))
  }
}

# dbExecute(con_sdm, "ALTER TABLE cell ADD PRIMARY KEY (cell_id);")
# dbExecute(con_sdm, "ALTER TABLE dataset ALTER COLUMN date_created SET DEFAULT CURRENT_DATE;")
# dbExecute(con_sdm, "ALTER TABLE metric ALTER COLUMN metric_seq SET DEFAULT nextval('metric_metric_seq_seq');")
# dbExecute(con_sdm, "ALTER TABLE metric ADD PRIMARY KEY (metric_seq);")
# dbExecute(con_sdm, "ALTER TABLE model ALTER COLUMN mdl_seq SET DEFAULT nextval('model_mdl_seq_seq');")
# dbExecute(con_sdm, "ALTER TABLE model ADD PRIMARY KEY (mdl_seq);")

## -- CREATE TABLE metric(metric_seq INTEGER DEFAULT(nextval('metric_metric_seq_seq')) PRIMARY KEY, metric_type VARCHAR NOT NULL, description VARCHAR, date_created DATE DEFAULT(CURRENT_DATE));;
## -- CREATE TABLE model(mdl_seq INTEGER DEFAULT(nextval('model_mdl_seq_seq')) PRIMARY KEY, ds_key VARCHAR, taxa VARCHAR, time_period VARCHAR, region VARCHAR, mdl_type VARCHAR, description VARCHAR, date_created DATE DEFAULT(CURRENT_DATE), FOREIGN KEY (ds_key) REFERENCES dataset(ds_key));;

seqs_exist <- dbGetQuery(con_sdm, "SELECT * FROM duckdb_sequences()") |> pull(sequence_name)

schema_sql <- glue("{dir_pq}/schema.sql")
for (ln in readLines(schema_sql)){ # ln = readLines(schema_sql)[1]
  if (str_detect(ln, "CREATE TABLE")) {
    # tbl <- str_extract(ln, "(?<=CREATE TABLE )[a-z_]+")
    # if (!dbExistsTable(con_sdm, tbl))
    #   dbExecute(con_sdm, ln)
    next
  } else if (str_detect(ln, "CREATE INDEX")) {
    # dbExecute(con_sdm, ln)
    next
  } else if (str_detect(ln, "CREATE SEQUENCE")) {
    seq <- str_extract(ln, "(?<=CREATE SEQUENCE )[a-z_]+")
    tbl <- str_replace(seq, "([[:alnum:]]+)_.*", "\\1")
    col <- str_replace(seq, glue("{tbl}_([[:alnum:]]+)_seq"), "\\1")
    
    
    if (!seq %in% seqs_exist){
        # message(glue("Sequence {seq} for table.column {tbl}.{col}: \n\t{ln}"))
        message(ln)
        dbExecute(con_sdm, ln)
    }
    
    sql <- glue("ALTER TABLE {tbl} ALTER COLUMN {col} SET DEFAULT nextval('{seq}');")
    message(sql)
    dbExecute(con_sdm, sql)
  }
}

```


## Close Connection

```{r cleanup}
dbDisconnect(con_sdm, shutdown = T)
```
